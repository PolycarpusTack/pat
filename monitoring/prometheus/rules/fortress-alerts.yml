# =============================================================================
# Pat Fortress - Prometheus Alerting Rules
# Production Alert Rules for Infrastructure and Applications
# =============================================================================

groups:

# =============================================================================
# Infrastructure Alerts
# =============================================================================

- name: infrastructure
  interval: 30s
  rules:

  # High CPU Usage
  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
    for: 5m
    labels:
      severity: warning
      category: infrastructure
      service: system
    annotations:
      summary: "High CPU usage detected"
      description: "CPU usage is above 85% on {{ $labels.instance }} (current value: {{ $value }}%)"
      runbook_url: "https://fortress.pat.local/runbooks/high-cpu"

  - alert: CriticalCPUUsage
    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
    for: 2m
    labels:
      severity: critical
      category: infrastructure
      service: system
    annotations:
      summary: "Critical CPU usage detected"
      description: "CPU usage is above 95% on {{ $labels.instance }} (current value: {{ $value }}%)"
      runbook_url: "https://fortress.pat.local/runbooks/critical-cpu"

  # High Memory Usage
  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
    for: 5m
    labels:
      severity: warning
      category: infrastructure
      service: system
    annotations:
      summary: "High memory usage detected"
      description: "Memory usage is above 85% on {{ $labels.instance }} (current value: {{ $value }}%)"

  - alert: CriticalMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
    for: 2m
    labels:
      severity: critical
      category: infrastructure
      service: system
    annotations:
      summary: "Critical memory usage detected"
      description: "Memory usage is above 95% on {{ $labels.instance }} (current value: {{ $value }}%)"

  # Disk Space Usage
  - alert: HighDiskUsage
    expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
    for: 5m
    labels:
      severity: warning
      category: infrastructure
      service: storage
    annotations:
      summary: "High disk usage detected"
      description: "Disk usage is above 85% on {{ $labels.instance }} mountpoint {{ $labels.mountpoint }} (current value: {{ $value }}%)"

  - alert: CriticalDiskUsage
    expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 95
    for: 2m
    labels:
      severity: critical
      category: infrastructure
      service: storage
    annotations:
      summary: "Critical disk usage detected"
      description: "Disk usage is above 95% on {{ $labels.instance }} mountpoint {{ $labels.mountpoint }} (current value: {{ $value }}%)"

  # Node Down
  - alert: NodeDown
    expr: up{job="node-exporter"} == 0
    for: 1m
    labels:
      severity: critical
      category: infrastructure
      service: system
    annotations:
      summary: "Node is down"
      description: "Node {{ $labels.instance }} has been down for more than 1 minute"

# =============================================================================
# Database Alerts
# =============================================================================

- name: database
  interval: 30s
  rules:

  # PostgreSQL Down
  - alert: PostgreSQLDown
    expr: pg_up == 0
    for: 1m
    labels:
      severity: critical
      category: database
      service: postgresql
    annotations:
      summary: "PostgreSQL is down"
      description: "PostgreSQL instance {{ $labels.instance }} is down"

  # High Database Connections
  - alert: PostgreSQLHighConnections
    expr: sum by(instance) (pg_stat_activity_count) / sum by(instance) (pg_settings_max_connections) * 100 > 80
    for: 5m
    labels:
      severity: warning
      category: database
      service: postgresql
    annotations:
      summary: "High PostgreSQL connections"
      description: "PostgreSQL connections are above 80% on {{ $labels.instance }} ({{ $value }}%)"

  # Database Replication Lag
  - alert: PostgreSQLReplicationLag
    expr: pg_stat_replication_lag_seconds > 300
    for: 2m
    labels:
      severity: warning
      category: database
      service: postgresql
    annotations:
      summary: "PostgreSQL replication lag is high"
      description: "Replication lag is {{ $value }} seconds on {{ $labels.instance }}"

  # Slow Queries
  - alert: PostgreSQLSlowQueries
    expr: increase(pg_stat_user_tables_n_tup_ins[5m]) + increase(pg_stat_user_tables_n_tup_upd[5m]) + increase(pg_stat_user_tables_n_tup_del[5m]) > 1000
    for: 5m
    labels:
      severity: warning
      category: database
      service: postgresql
    annotations:
      summary: "High PostgreSQL query rate"
      description: "High query rate detected on {{ $labels.instance }}: {{ $value }} queries/5min"

  # Redis Down
  - alert: RedisDown
    expr: redis_up == 0
    for: 1m
    labels:
      severity: critical
      category: database
      service: redis
    annotations:
      summary: "Redis is down"
      description: "Redis instance {{ $labels.instance }} is down"

  # High Redis Memory Usage
  - alert: RedisHighMemoryUsage
    expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 85
    for: 5m
    labels:
      severity: warning
      category: database
      service: redis
    annotations:
      summary: "High Redis memory usage"
      description: "Redis memory usage is above 85% on {{ $labels.instance }} ({{ $value }}%)"

# =============================================================================
# Application Alerts
# =============================================================================

- name: applications
  interval: 30s
  rules:

  # Application Down
  - alert: FortressApplicationDown
    expr: up{job=~"fortress-.*"} == 0
    for: 2m
    labels:
      severity: critical
      category: application
      service: "{{ $labels.job }}"
    annotations:
      summary: "Fortress application is down"
      description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 2 minutes"

  # High HTTP Error Rate
  - alert: HighHTTPErrorRate
    expr: sum(rate(http_requests_total{status=~"5.."}[5m])) by (service) / sum(rate(http_requests_total[5m])) by (service) * 100 > 5
    for: 5m
    labels:
      severity: warning
      category: application
      service: "{{ $labels.service }}"
    annotations:
      summary: "High HTTP 5xx error rate"
      description: "HTTP 5xx error rate is above 5% for {{ $labels.service }} (current: {{ $value }}%)"

  # High Response Time
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)) > 2
    for: 5m
    labels:
      severity: warning
      category: application
      service: "{{ $labels.service }}"
    annotations:
      summary: "High HTTP response time"
      description: "95th percentile response time is above 2s for {{ $labels.service }} (current: {{ $value }}s)"

  # SMTP Server Issues
  - alert: SMTPHighConnectionFailures
    expr: rate(smtp_connection_failures_total[5m]) > 0.1
    for: 3m
    labels:
      severity: warning
      category: application
      service: smtp
    annotations:
      summary: "High SMTP connection failures"
      description: "SMTP connection failure rate is {{ $value }} failures/second"

  - alert: SMTPHighProcessingTime
    expr: histogram_quantile(0.95, sum(rate(smtp_message_processing_seconds_bucket[5m])) by (le)) > 10
    for: 5m
    labels:
      severity: warning
      category: application
      service: smtp
    annotations:
      summary: "High SMTP message processing time"
      description: "95th percentile SMTP processing time is {{ $value }} seconds"

  # Email Queue Backlog
  - alert: EmailQueueBacklog
    expr: email_queue_size > 10000
    for: 5m
    labels:
      severity: warning
      category: application
      service: email-processing
    annotations:
      summary: "Email queue backlog detected"
      description: "Email queue has {{ $value }} pending messages"

  # GraphQL Query Complexity
  - alert: HighGraphQLQueryComplexity
    expr: histogram_quantile(0.95, sum(rate(graphql_query_complexity_bucket[5m])) by (le)) > 1000
    for: 5m
    labels:
      severity: warning
      category: application
      service: graphql-api
    annotations:
      summary: "High GraphQL query complexity"
      description: "95th percentile GraphQL query complexity is {{ $value }}"

# =============================================================================
# Security Alerts
# =============================================================================

- name: security
  interval: 30s
  rules:

  # Too Many Failed Login Attempts
  - alert: HighFailedLoginAttempts
    expr: increase(failed_login_attempts_total[5m]) > 50
    for: 2m
    labels:
      severity: warning
      category: security
      service: authentication
    annotations:
      summary: "High number of failed login attempts"
      description: "{{ $value }} failed login attempts in the last 5 minutes from {{ $labels.source_ip }}"

  # Suspicious Activity
  - alert: SuspiciousAPIActivity
    expr: increase(http_requests_total{status="403"}[5m]) > 100
    for: 3m
    labels:
      severity: warning
      category: security
      service: api
    annotations:
      summary: "Suspicious API activity detected"
      description: "{{ $value }} 403 Forbidden responses in the last 5 minutes"

  # Certificate Expiration
  - alert: SSLCertificateExpiringSoon
    expr: (ssl_certificate_expiry_days < 30) > 0
    for: 1h
    labels:
      severity: warning
      category: security
      service: tls
    annotations:
      summary: "SSL certificate expiring soon"
      description: "SSL certificate for {{ $labels.domain }} expires in {{ $value }} days"

  - alert: SSLCertificateExpiring
    expr: (ssl_certificate_expiry_days < 7) > 0
    for: 1h
    labels:
      severity: critical
      category: security
      service: tls
    annotations:
      summary: "SSL certificate expiring very soon"
      description: "SSL certificate for {{ $labels.domain }} expires in {{ $value }} days"

# =============================================================================
# Business Logic Alerts
# =============================================================================

- name: business
  interval: 60s
  rules:

  # Plugin Execution Failures
  - alert: HighPluginFailureRate
    expr: sum(rate(plugin_executions_failed_total[10m])) by (plugin) / sum(rate(plugin_executions_total[10m])) by (plugin) * 100 > 10
    for: 10m
    labels:
      severity: warning
      category: business
      service: plugin-runtime
    annotations:
      summary: "High plugin failure rate"
      description: "Plugin {{ $labels.plugin }} has a failure rate of {{ $value }}%"

  # Workflow Failures
  - alert: WorkflowExecutionFailures
    expr: increase(workflow_executions_failed_total[15m]) > 10
    for: 5m
    labels:
      severity: warning
      category: business
      service: workflow-engine
    annotations:
      summary: "High workflow failure rate"
      description: "{{ $value }} workflow executions failed in the last 15 minutes"

  # Email Delivery Issues
  - alert: LowEmailDeliveryRate
    expr: sum(rate(emails_delivered_total[10m])) / sum(rate(emails_processed_total[10m])) * 100 < 95
    for: 10m
    labels:
      severity: warning
      category: business
      service: email-delivery
    annotations:
      summary: "Low email delivery rate"
      description: "Email delivery rate is {{ $value }}% (below 95% threshold)"

  # Storage Quota Alerts
  - alert: EmailStorageQuotaHigh
    expr: email_storage_used_bytes / email_storage_quota_bytes * 100 > 85
    for: 5m
    labels:
      severity: warning
      category: business
      service: storage
    annotations:
      summary: "Email storage quota usage high"
      description: "Email storage usage is {{ $value }}% of quota"

# =============================================================================
# Kubernetes Alerts (when running on K8s)
# =============================================================================

- name: kubernetes
  interval: 30s
  rules:

  # Pod Crashes
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: warning
      category: kubernetes
      service: "{{ $labels.container }}"
    annotations:
      summary: "Pod is crash looping"
      description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ $value }} times / 15 minutes"

  # Node Not Ready
  - alert: NodeNotReady
    expr: kube_node_status_condition{condition="Ready",status="true"} == 0
    for: 5m
    labels:
      severity: critical
      category: kubernetes
      service: cluster
    annotations:
      summary: "Kubernetes node not ready"
      description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"

  # Pod Pending
  - alert: PodPending
    expr: kube_pod_status_phase{phase="Pending"} == 1
    for: 10m
    labels:
      severity: warning
      category: kubernetes
      service: scheduling
    annotations:
      summary: "Pod stuck in pending"
      description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been pending for more than 10 minutes"

  # Deployment Rollout Stuck
  - alert: DeploymentRolloutStuck
    expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
    for: 15m
    labels:
      severity: warning
      category: kubernetes
      service: deployment
    annotations:
      summary: "Deployment rollout stuck"
      description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} rollout has been stuck for more than 15 minutes"